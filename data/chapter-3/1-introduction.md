---
path: '/chapter-3/1-introduction'
title: 'Introduction'
hidden: false
---

<chapter-box>

1. Introduction
2. What is “accountability”? Agents, acts and omissions
3. Moral Machines

</chapter-box>

In the city of Amsterdam, parking control services are partially automated. The service automises
the process of license plate identification and background checks with specific scanning equipment
and object recognition based identification service. The automated parking control service is
currently in use across 150,000 street parking spaces in the city. Service follows a three step
process. In the first step, scan cars equipped with cameras drive through the city and use object
recognition software to scan and identify the license plates of surrounding cars. After the
identification, the license plate number is checked against the National Parking Register to validate
if the car has a permission to park at a given location. Whenever no payment has been made for
current parking, the case is provided to a human inspector for further processing. In the last step,
parking inspector uses scanned images to remotely assess whether there is a special situation such
as loading or unloading, or stationary cars in front of a traffic light. Parking inspector may also
verify the situation on-site by scooter. Whenever there is no valid reason for non-paid parking, a
parking ticket is issued.

Parking control services provide an example of how algorithms are increasingly used for automating public services.  As they are exact, fast and precise, algorithms often promote efficiency, reliability and consistency of services.  Paradoxically, algorithms can also make systematic errors, be biased and cause serious harms. For example, scanning systems may malfunction, or suffer from bugs. They may make mistakes, and suggest the tickets be issued on invalid grounds. In these cases, who should take the responsibility – and on what grounds?

Although we say things like "yes, it was the algorithm’s fault and it is responsible for the wrong decision", we do not literally mean that contemporary algorithms would be morally guilty. Instead, the algorithms are causal factors that underlie the decisions. Mere causes, however, differ from morally responsible actions.

Although algorithms themselves cannot be held accountable as they are not moral or legal agents, the organizations designing and deploying algorithms can be taken to be morally responsible  through governance structures. Thus, in the case of the city of Amsterdam, it is the human inspector that makes the final decision – and also takes responsibility. However, one day the human inspector may be replaced by algorithms, too. Who, then, would take responsibility?


<text-box icon="techIcon" name="Automated vs. autonomous decision making">

**Automated systems** typically run within a well-defined set of parameters and are very
restricted in what tasks they can perform. The decisions made or actions taken by an
automated system are based on predefined heuristics.

**An autonomous system** learns and adapts to dynamic environments, and evolves as the
environment around it changes. The data it learns and adapts to may be outside what was
contemplated when the system was deployed.

Automation or autonomisation comes into degree, and hence, they are continuums rather
than on/off dichotomies. For example, a system can be said to be autonomous with
respect to human control to a certain degree.

  **- picture of levels of automatisation -**

</text-box>
