---
path: '/chapter-3/1-introduction'
title: 'Introduction'
hidden: false
---

<text-box variant='Intro' name='Learning objectives'>

1. INTRODUCTION
2. What is “accountability”? Agents, acts and omissions
3. Moral Machines

</text-box>

In the city of Amsterdam, parking control services are partially automated. The service automises
the process of license plate identification and background checks with specific scanning equipment
and object recognition based identification service. The automated parking control service is
currently in use across 150,000 street parking spaces in the city. Service follows a three step
process. In the first step, scan cars equipped with cameras drive through the city and use object
recognition software to scan and identify the license plates of surrounding cars. After the
identification, the license plate number is checked against the National Parking Register to validate
if the car has a permission to park at a given location. Whenever no payment has been made for
current parking, the case is provided to a human inspector for further processing. In the last step,
parking inspector uses scanned images to remotely assess whether there is a special situation such
as loading or unloading, or stationary cars in front of a traffic light. Parking inspector may also
verify the situation on-site by scooter. Whenever there is no valid reason for non-paid parking, a
parking ticket is issued.

Parking control services provide an example of how algorithms are increasingly used for
automating decision making. As being exact, fast and precise, algorithms often promote
efficiency, reliability and consistency of decisions. Paradoxically, algorithms can also be make
systematic errors, be biased and cause serious harms. Moreover, automated decision making
raises a fundamental ethical question of the accountability: If we use algorithms for decision
making, who takes the responsibility - especially if the algorithms make mistakes? What are the
requirements for acceptable use of such algorithms?

The parking control services provides an example of partially automated decision making, since it
is the human inspector who makes the final decision about issuing the parking ticket. However,
one day it may be possible to replace the human inspectors by algorithms, too. In this case, the
decision making is said to be completely automated. Would the algorithms, then, be responsible
for their decisions, and on what grounds?

<text-box variant="hint" name="Automated vs. autonomous decision making">

Automated systems typically run within a well-defined set of parameters and are very
restricted in what tasks they can perform. The decisions made or actions taken by an
automated system are based on predefined heuristics.

An autonomous system learns and adapts to dynamic environments, and evolves as the
environment around it changes. The data it learns and adapts to may be outside what was
contemplated when the system was deployed.

Automation or autonomisation comes into degree, and hence, they are continuums rather
than on/off dichotomies. For example, a system can be said to be autonomous with
respect to human control to a certain degree.

  **- picture of levels of automatisation -**

</text-box>
