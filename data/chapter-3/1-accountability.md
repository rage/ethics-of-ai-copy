---
path: '/chapter-3/1-accountability'
title: 'Accountability: Who should be blamed?'
hidden: false
---



<text-box variant='Intro' name='Learning objectives'>

1.  Defining accountability
2.  Agents, Actions and Omissions
3.  Moral machines

</text-box>

## **1. Defining accountability**

From estimating taxation to determining social benefits or health insurance premiums, AI and algorithmic systems in general are increasingly being used for decision-making. Algorithms can be used to promote fairness, equality and well-being.  Paradoxically, they can also discriminate,  violate human rights and be unfair.

 [Examples](https://ethics-of-ai.now.sh/)

Obviously, current AI applications are just computer programs, not morally responsible agents (they have no moral agency by themselves, meaning they are not capable of acting with reference to right and wrong). Instead, the moral abilities of contemporary AI systems are based on the users’ or developers’ intentions and aims.
That said, algorithms still raise a growing challenge for legislators, authorities and policy makers in many countries. They force us to ask the question: if algorithmic systems make mistakes, who should we blame – and on what grounds?

<text-box variant="hint" name="Definitions, definitions">
An automated systems typically runs within a well-defined set of parameters and are very restricted in what tasks they can perform. The decisions made or actions taken by an automated system are based on predefined heuristics.

An autonomous system learns and adapts to dynamic environments, and evolves as the environment around it changes. The data it learns and adapts to may be outside what was contemplated when the system was deployed.

Data-based decision making uses collected data to make decisions. This can be done either automatically or by a human analyst.
</text-box>

