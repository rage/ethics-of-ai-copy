---
path: '/chapter-3/3-moral-machines'
title: 'Moral machines'
hidden: false
---

However, even if present-day AI systems lack moral status in terms of moral agents, it is still a theoretical possibility that, some day, future AI systems may have such a status. Because of this, many have addressed the issue of “automated moral reasoning” or the question of possible development of genuinely moral artificial agents.
In their famous study, researchers from MIT… [Moral Machine](https://www.moralmachine.net/).
From a philosophical point of view, the MIT experiment is based on the so-called trolley problem. It is a classical philosophical thought experiment. Trolley problems involve a choice between the harm/benefit of one person versus many people (e.g. Wallach and Allen 2008, 14).  As such, Moral Machine is a platform for gathering a human perspective on moral decisions made by machine intelligence, such as self-driving cars.
Strictly speaking, Moral Machine gathers data only on preferences, not evaluations on harms or benefits.
Please, visit: link.
However, as several critics have pointed out, there are several important differences between real world and trolley scenarios (Kauppinen, under evaluation). First, in trolley cases the outcome of each choice is stipulated to be certain, while in real life the outcome of a choice is always more or less uncertain (Nyholm and Smids 2015; Goodall 2016). In other words, in real world situations, trolley problems are about the posing of a risk rather than causing harm.
Second, as Nyholm and Smids (2015) emphasize, trolley cases involve an agent’s response in a one-off, real-time situation, while the question for autonomous cars concerns pre-set general algorithms, so that agency is exercised in advance of any emergency situation, and by those who program the car to react in a certain way.
Third, in philosophical trolley scenarios each potential victim is entirely innocent in the sense that by stipulation. This is not always the case in unavoidable crash situations in real life.
