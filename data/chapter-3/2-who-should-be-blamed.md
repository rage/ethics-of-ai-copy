---
path: '/chapter-3/2-who-should-be-blamed'
title: 'Who should be blamed?'
hidden: false
---

# **Who should be blamed – and for what?**


**Moral agency**

In ethics, a moral agent is often characterized as “an agent who is capable of acting with reference to right and wrong." Importantly, only moral agents are morally responsible for their actions.  Although we say things like "the algorithm is responsible for this decision", we do not literally mean that algorithms would be morally guilty. Instead, the algorithms may cause the decision. Mere causes, however, differ from morally responsible actions.

According to philosophers, moral responsibility requires both moral autonomy and the ability to evaluate the consequences of actions. “Moral autonomy” means the agent has capacity to impose a moral code on oneself in a self-governed way.  Furthermore, autonomy requires the capacity to rule oneself without manipulation by others and the ability to act without external or internal constraints (see Dworkin 1989). It also emphasizes the authenticity of the desires (values, emotions, etc) that move one to act. [Link to literature](https://ethics-of-ai.now.sh/).

Moral autonomy requires that an actor has sufficient cognitive skills. They  must be able to evaluate, to predict and to compare consequences. An actor must be able to estimate motives that drive action. But these thinking skills are measured in degree, and hence, cognitively, autonomy is not an on/off issue. For example, a human and a machine can form a hybrid agent, where  a machine can be said to be autonomous to certain degree (Müller 2012).

<text-box variant="hint" name="Moral responsibility">

Immanuel Kant is one of the most famous moral philosophers in Western thought. For Immanuel Kant, practical reason — our ability to use reasons to choose our own actions — presupposes that we are free.  It means that actions are based on our own will to utilize a moral law to guide our decisions. For Kant, and Kantians, this capacity to impose upon ourselves moral law is the ultimate source of all moral value.

So, according to Kant, we owe to ourselves moral respect in virtue of our autonomy. But insofar as this capacity depends in no way on anything particular or contingent about ourselves, we owe similar respect to all other persons in virtue to their capacity. That means (via the second formulation of Kant´s famous [Categorical Imperative](https://en.wikipedia.org/wiki/Categorical_imperative)), we are obliged to act out of fundamental respect for other persons in virtue of their autonomy. In this way, autonomy serves as both a model of practical reason in the determination of moral obligation and as the feature of other persons deserving moral respect from us. (For further discussion, see Immanual Kant and moral philosophy.)

<text-box>

**Actions and omissions**

From a moral point of view, a moral agent is primarily responsible for their own actions (“acts”). Sometimes agents are also responsible for actions which they do not actually do. These are called “omissions”.

Obviously, agents cannot be responsible for all of the things they do not do. That’s why  philosophers emphasize that agents are responsible for only those things which they´ve deliberately chosen to do or to omit. However, omissions and actions are not morally equal. It is morally less bad to omit a thing than to perform an act. In other words, it’s simply worse to actively kill someone than to let them die.

<text-box variant="hint" name="A story">

Helsinki  -- is it moral not to help people by using AI-methods, even if those methods may violate the privacy? Can you help someone against their own will?

<text-box>

**Accountability and transparency**

Who should we blame when AI systems don’t work as intended or make a mistake? Accountability is a relative notion: an actor is responsible for a specific action or omission, but the quality of responsibility is dependent on the stakeholder. We are not universally responsible. But it has turned out to be really difficult to develop a set of criteria for specific responsibilities. In many countries, there is an on-going debate on this question. Many international actors, such as European Union and G7 have addressed this as an open challenge (REFS).

<text-box variant="hint" name="Different approaches">

Accountability is related to other issues, such as transparency ([Chapter 4](https://ethics-of-ai.now.sh/). Accountability is primarily a legal and ethical obligation on an individual (or organisation) to accept responsibility for the use of AI systems, and to disclose the results in a transparent manner.

<text-box>

Moral machines
However, even if present-day AI systems lack moral status in terms of moral agents, it is still a theoretical possibility that, some day, future AI systems may have such a status. Because of this, many have addressed the issue of “automated moral reasoning” or the question of possible development of genuinely moral artificial agents.
In their famous study, researchers from MIT… [Moral Machine](https://www.moralmachine.net/).
From a philosophical point of view, the MIT experiment is based on the so-called trolley problem. It is a classical philosophical thought experiment. Trolley problems involve a choice between the harm/benefit of one person versus many people (e.g. Wallach and Allen 2008, 14).  As such, Moral Machine is a platform for gathering a human perspective on moral decisions made by machine intelligence, such as self-driving cars.
Strictly speaking, Moral Machine gathers data only on preferences, not evaluations on harms or benefits.
Please, visit: link.
However, as several critics have pointed out, there are several important differences between real world and trolley scenarios (Kauppinen, under evaluation). First, in trolley cases the outcome of each choice is stipulated to be certain, while in real life the outcome of a choice is always more or less uncertain (Nyholm and Smids 2015; Goodall 2016). In other words, in real world situations, trolley problems are about the posing of a risk rather than causing harm.
Second, as Nyholm and Smids (2015) emphasize, trolley cases involve an agent’s response in a one-off, real-time situation, while the question for autonomous cars concerns pre-set general algorithms, so that agency is exercised in advance of any emergency situation, and by those who program the car to react in a certain way.
Third, in philosophical trolley scenarios each potential victim is entirely innocent in the sense that by stipulation. This is not always the case in unavoidable crash situations in real life.
