---
path: '/chapter-3/4-the-problem-of-individuating-responsibilities'
title: 'The problem of individuating responsibilities'
hidden: false
---

<hero-icon heroIcon='chap3'/>


<styled-text>

Accountability is often taken as a legal and ethical obligation on an individual or organisation to accept responsibility for the use of AI systems, and to disclose the results in a transparent manner. This formulation presupposes a “power-relation”. It individuates who is in control and who is to be blamed.

However, it has turned out to be notoriously difficult to set specific criteria on how, exactly, the responsibilities should be individuated, directed and defined. In many countries, there are on-going debates on these questions. International actors, such as European Union and G7 have addressed them as  open challenges.

Why is it so difficult to set criteria on who is responsible?

* **Firstly**, the quality of responsibilities differ. An actor is responsible for a specific action or omission, but the quality of responsibility is dependent on the stakeholder. Thus, although by choosing an action you may commit the responsibility, the quality of responsibility is dependent also on your properties. Intelligent technologies complicate this more.

    As we delegate more and more decision making tasks and functions to algorithms, we also shape decision making structures. AI is augmenting our intelligence by giving us more computational power, allowing better predictions and enhancing our sensory apparatus. Human and machines become cognitive hybrids.  They cooperate cognitively (thinking) and epistemically (knowledge), both at the individual and collective level. This creates systemic properties.

    It is often thought that it is sufficient that a human stays “in-the-loop” or “on-the-loop” – meaning at  some point of decision making, a human individual would be able to monitor or intervene in the artificial system. However, as algorithms enter into decision making, say, in public sector governance, the collective decision making can take a very complex and highly distributed form. To individuate and address the factors in a way that would guarantee that a human stays in/on-the-loop may be really difficult.

* **Secondly**, technology can also take the persuasive form: it influences and controls people.  A classical example is the beeping sound of seat belts. In many cars, if the seat belts are not fastened, it will cause a constant beeping sound. This can be taken as a form of controlling influence—in this case a kind of coercion. The driver can only stop the sound by fastening the belt. Contemporary algorithmic applications can have more and more such features; they propose, suggest and limit the options.

    But, an action is done voluntarily only if the action is done intentionally (the one acting is “in control”) and is free from controlling influences. Is the driver free from controlling influences, if the seat belt system forces him to react to the beeping sound? Or, are we free from control, if the algorithms decide whose pictures we´ll see in the dating sites, or what music we are about to listen to? What, exactly, is the difference between algorithmic suggestion, control, or manipulation?

    Naturally, persuasive technology should comply with the requirement of voluntariness to guarantee autonomy. Algorithms complicate this issue, since the voluntariness presupposes a sufficient understanding of the use of specific technology. But, what does it mean to “understand”, and what is the sufficient degree, really? What is the correct reading of “understandability” – “transparency”, “explainability” or  “auditability”? How much, and what, exactly, a user should understand about the technology? When can one genuinely  estimate, whether or not they want to use that particular technology? We’ll look at this topic in more detail in chapter 4.

</styled-text>

<quiz id="63998353-c785-496e-a447-7f61202ecf3a"> </quiz>

<quiz id="b6f98b77-1ca1-4c74-b0ad-d2f373c4c9db">


<img src="_MS_9489_HDR_cropped.jpg" alt="Hospital"> </img>

Picture © City of Helsinki / Communications

<br>
<br>

Let’s return to the case of healthcare in Helsinki (as mentioned at the start of chapter 2). Suppose you are the Chief Digital Officer in Helsinki City. You are asked to consider whether the city’s healthcare organization should move from reactive healthcare to preventive healthcare. You read a report. It discusses novel machine learning-based methods, which would help health authorities to forecast the possible health risks of citizens.

 <br>

The report mentions many advantages, such as sickness prevention, better impact estimation, and better planning of basic healthcare services. However, the report also reveals some concerns over privacy, polarization, and the possible threat of unintentional discrimination. Moreover, the report raises the fundamental question of the city’s role. If the city has information about the potential health risks and does not act upon the data, is the city morally responsible for negligence?

<br>

The report also addresses the issue of individuating responsibilities. If the system was applied in practice, there is always a risk that it would make a mistake. Who should we blame if this happens?

<br>

Please answer the following questions about the responsibility of a Chief Digital Officer:

</quiz>
