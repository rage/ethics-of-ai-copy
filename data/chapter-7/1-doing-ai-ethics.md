---
path: '/chapter-7/1-doing-ai-ethics'
title: 'Doing AI ethics'
hidden: false
---

So far we have looked at core ethical principles around which AI ethics conversations currently revolve. In this section, we will step away from the contents of ethical statements to take a wider perspective into the whole project of AI ethics. This means looking at the /making/ and /doing/ of AI ethics as a cultural phenomenon, a project with diverse members with differing stakes, and producing texts that can be analysed as cultural products. That is, AI Ethics is created by certain people, at a certain time, for certain purposes.

## From principles to doing

“[Ethics] plays the role of a bicycle brake on an intercontinental airplane”
-Ulrike Beck, 1988

In this course, we have approached Al ethics with a systematic normative approach. AI ethics is based on an evolving framework of interdependent values, principles, and actions that can guide societies in dealing responsibly with the impacts of Al technologies on human beings, societies, and the environment. Rather than equating ethics to law, or as a normative add-on to technologies, ethics is considered as a conceptual compass or a tool for the normative evaluation and guidance of Al technologies.

<text-box variant="hint">

To date, most ethical discussions regarding AI systems have focused on defining principles to prevent risks. National policies and various initiatives have been made on the basis of them.

AlgorithmWatch, a Berlin-based organization, maintains a repository of published AI ethics guidelines, with over 160 collected so far. It is hard to find any actor in AI these days that does not point to their own set of guidelines as evidence of their ethical engagement. Although ethical principles can shape the development and implementation of ethics-based policy measures and legal norms, empirical studies suggest that guidelines have little impact on the practices surrounding AI development:

“Despite its stated goal, we found no evidence that the ACM code of ethics influences ethical decision-making. Future research is required to identify interventions that do influence decision-making, such as by helping developers identify parallels between their decisions and infamous software news stories.”
McNamara et al. 2018

</text-box>

Often there is a lack of real implementation mechanics and assessment practices that would turn guidelines into more ethically aware development and, on the other hand, mechanisms to halt ethically suspect projects. This has led some critics to point to many of the activities around the discussion of AI ethics as simply “ethics-washing”.

II. Ethics as doing


<text-box>

AuroraAI case here.

</text-box>


AuroraAI illustrates how the discourse around AI ethics principles and their technical manifestations is often based on an overly narrow and technologized interpretation of concepts like “justice”.  The fundamental ethical questions on the principles of good governance, or the impact of technology on the relationship between a state and its citizens, cannot be fixed by technical measures alone.

The AuroraAI project crystallizes how the reduction of ethical questions into narrow technologized concepts doesn’t answer questions posed by the developers of the project. Instead, the reduction obscures the larger socio-technical issues and the social orderings brought about by an AI future. For example, remembering our discussion on fairness in chapter 6, the effort to de-bias datasets by adding more diverse representation may fulfill a mathematical definition of fairness. But if that dataset only serves to create more effective tools of surveillance and subjugation, then our conception of ethics has failed to capture what it means to be just. Precisely the same applies to many other examples.

### A shared project
The search for technical fixes to issues like fairness and privacy paint a picture of AI as a shared, collective project: an ideal future which humanity cooperatively strives for, encountering technical obstacles to ethics that can be overcome with technical fixes. This is evidenced most clearly in the positioning of AI as something outside of social relations, and the depiction of humanity as a unanimous polity, in phrases like “AI should respect human values” or “the human aspect in AI”. A more appropriate image positions AI, like any other technology, in the spaces between human collectives and into the struggles and power relations among us. This means also admitting that there are plainly malicious actors and explicitly malicious uses of AI:

“These occur in various areas (Pistono and Yampolskiy 2016; Amodei et al. 2017), such as in the military use of AI in cyber warfare or regarding weaponized unmanned vehicles or drones (Ernest and Carroll 2016; Anderson and Waxman 2013). Moreover, AI applications are already being used for automated propaganda and disinformation campaigns (Lazer et  al. 2018), social control (Engelmann et  al. 2019), surveillance (Helbing 2019), face recognition or sentiment analysis (Introna and Wood 2004), social sorting (Lyon 2003), or improved interrogation techniques (McAllister 2017).”
Hagendorff (2020)

In this section, we will take a wider look at the whole project of AI ethics. This means looking at the making and doing of AI ethics as a phenomenon. AI ethics is a project with diverse members who have differing stakes – and it produces contents and discussions that can be analyzed as cultural products. That is, AI ethics is created by certain people, at a certain time, for certain purposes. But what are these purposes, and is AI ethics achieving its goals?

The subject of ethical considerations relating to technology is not a new question. Rather, it has been asked in a very similar form throughout many different iterations of technological development. As an illustration of the historical continuities around these issues, the sociologist Langdon Winner wrote in his 1990 essay “Engineering Ethics and Political Imagination” that a difficulty lies in ethical discussions centring on highly hypothetical and limited “troubling incidents” – without calling into question the broader responsibilities of the whole engineering industry. This issue is similar to those facing the AI ethics project now, 30 years later.

While the troubles may not be new, the contemporary AI ethics project has much more public interest and wider stakeholder participation than engineering ethics has had before. Many more are learning about it, many are hoping to implement AI ethics in their work in some way, and many companies, communities, states and individuals have stakes in the outcomes. With this new wave of interest, publishing ethical guidelines has become the typical way of doing ethics.

Ethics guidelines as texts that do

<text-box name="Performativity">

First coined by philosopher John L. Austin, but made more famous by feminist scholar Judith Butler, performativity is the capacity of words to do things in the world. That is, sometimes making statements does not just describe the world, but also performs a social function beyond describing. As an example (from Austin), when a priest declares a couple to be “husband and wife”,  they are not describing the state of their relationship. Rather by that very declaration, the institution of marriage is brought about. The words perform the marrying of the two people. In a similar way, other acts of communication can also have a performative quality – not just presenting information, but effecting change in the world.

</text-box>


To conceptualize AI ethics guidelines as performative texts invites us to look past the ethical contents of the guidelines, and look into what the text does as a communicative act. To do so we have to consider who the text is written by and who the intended audience is. Where it is published and what other authors and actors it references also play into what kind of ends are achieved by the text. Sociologists have looked into the performativity of ethical guidelines and found that they serve to create and manage expectations among different stakeholders in the AI economy.

Let’s look at three ways that guidelines perform these functions:

#### 1) Guidelines as calls for deregulation

Some authors have argued that ethical language deployed by companies is a communicative strategy that provides support for self-regulation (Wagner 2018). The implicit narrative in publishing ethics guidelines is that of a strong moral reflection at the core of business practices, which makes industry regulation unnecessary. Why should our business be restricted, since we are already so ethical? While this communicative strategy is more strongly motivated for private companies, national ethics guidelines  also reflect a tension between ethical consideration, regulation, and market-driven innovation. These conceptualize ethics as a value, to be put into healthy balance with competing values such as economic growth.

One of the core tensions in the AI economy is the balancing of the freedom to pursue projects and innovate versus the need to prevent and redress harm through regulatory practices. How the tension is resolved will undoubtedly have meaningful consequences for companies working in the AI scene, so it is far from an abstract issue. Thus, the concern for the regulation/deregulation debate is present not only in the practices of communication around AI ethics, but also in the contents of scientific research around fair AI. Computer scientist Michae Kearns, for example, presents technical advances in algorithmic fairness and privacy as answers to the regulation/deregulation debate.

#### 2) Guidelines as assurances

Others have argued that ethics guidelines work as assurance to investors and the public (Kerr 2020). That is, in the age of social media, news of businesses’ moral misgivings spread fast and can cause quick shifts in a company’s public image. Publishing ethics guidelines makes assurances that the organization has the competence for producing ethical language, and the capacity to take part in public moral discussions to soothe public concern.

Thus AI ethics guidelines work to deflect critique from companies: both from investors as well as the general public. That is, if the company is seen as being able to manage and anticipate the ethical critique produced by journalists, regulators and civil society, the company will also be seen as a stable investment, with the competence to navigate public discourses that may otherwise be harmful for the company’s outlook.


#### 3) Guidelines as expertise

With the AI boom well underway, the need for new kinds of expertise arises, and competition around ownership of the AI issue increases. That is, the negotiations around AI regulation, the creation of AI-driven projects of governmental redesign, the implementation of AI in new fields, and the public discourse around AI ethics in the news all demand expertise in AI and especially the intersection of AI and society.

To be seen as an expert yields certain forms of power. Experts create judgements, provide legitimacy to viewpoints, define situations, and set priorities. Thus, being seen as an AI ethics expert gives some say in what the future of society will look like.

Expertise is only effective if it is publicly recognized, and in the race to become a leading AI organization, publications are a way to secure visibility. According to Greene et al., “ethics and ethical codes designate and defend social status and expertise”. That is, taking part in the AI ethics discussion by publishing a set of ethical guidelines is a way to demonstrate expertise, increasing the organization’s chances of being invited to a seat at the table in regards to future AI issues.

Even if ethical guidelines are ineffectual in effecting change in AI practice, they can nonetheless be useful as communicative strategies and perform other functions for the actors involved. They can help us to figure out what kind of principles and practices should be taken up in the future of ethical AI. And they force us to ask how diverse organizations and individuals can take part in building a more just AI future in their own ways.

### III. Moving forward with ethics
Moving beyond ethical guidelines, how should AI ethics manifest in the future? What kind of conversations around the ethics of AI should we have, and what kind of activities and ways of doing ethics should be taken into practice? This is a difficult question to answer, but some hints can be found from looking at what is left outside the scope of the AI ethics guidelines we have been discussing so far.

Fairness, accountability, and transparency have come to dominate the AI ethics conversation. They also comprise the name of the largest scientific conference around ethical AI, FAccT. However, as AuroraAI illustrates, questions of “care, nurture, help, welfare, social responsibility or ecological networks” are the fundamental moral questions directed at any government that aims to deploy AI in its public sector services.

But, as HXX remarks, these values do not often appear in ethical guidelines. What, then, would it look like to take a perspective of care to AI ethics? According to the ethics of care, this means taking into consideration the complex dependencies and interdependencies between individuals, how the consequences of actions propagate and affect the most vulnerable, and how nature and ecology become entwined in these processes.

<text-box>

An example of this kind of mapping can be seen in the Anatomy of An AI System schematic created by researchers Crawford and Joler. The schematic shows the complex and complete interactions that go into creating an AI artefact. It expands outwards from just software to the material processes of creating an AI system; the markets, infrastructures and geological processes involved. This reveals ethically relevant aspects that would otherwise be obscured by the lens provided by AI ethics: the ecological damages of mining and the labor conditions of data processing, for example.

</text-box>


Moving from new perspectives to new practices, the role of citizens and civil society along with companies in the creation of more just AI should be examined. Creating ethical AI in practice means moving on from publishing good intentions to the many ways that societal actors can participate in effecting different futures.

<text-box>

Or, to take another example, let’s look at AlgorithmWatch. AlgorithmWatch tracks the wealth of public and private AI ventures in Europe, analyzes their consequences, and publishes information on how they are progressing and the many ways they are affecting society. The UK has also seen citizen action in the form of protests revolving around the use of predictive grading algorithms as a fix for the difficulties in organizing exams caused by the COVID-19 virus.

</text-box>


The Director of the Ada Lovelace institute Carly Kind calls this the third wave of AI ethics, and suggests we are moving into a new form of societal engagement:

“Third-wave ethical AI has seen a Dutch Court shut down an algorithmic fraud detection system, students in the UK take to the streets to protest against algorithmically-decided exam results, and US companies voluntarily restrict their sales of facial recognition technology. It is taking us beyond the principled and the technical, to practical mechanisms for rectifying power imbalances and achieving individual and societal justice.”
-Carly Kind

Conclusion: now it’s your turn
Ethical questions regarding Al Systems pertain to all stages of the Al system lifecycle, understood here to range from research, design, and development to deployment and use – including maintenance, operation, trade, financing, monitoring and evaluation, validation, end-of-use, disassembly, and termination.

In addition, Al actors can be defined as any actors involved in at least one stage of the Al lifecycle, and can refer both to natural and legal persons, such as researchers, programmers, engineers, data scientists, end users, large technology companies, small and medium enterprises, start-ups, universities, and public entities, among others.

<text-box>

Al systems raise ethical issues that include, but are not limited to, their impact on decision-making, equality, polarization, and well-being.

Al systems impact societal sectors such as employment and labor, social interaction, healthcare, education, weaponization, transport, and media.

AI systems cover topics such as freedom of expression, access to information, privacy, democracy, or discrimination.

AI systems can also change human experience, challenge human agency, raise concerns on the reliability of sources of information, and question the ideal of fundamental dignity.

</text-box>

Now, it is your turn to think about these questions.
