---
path: '/osa-2/1-ethical-concepts'
title: ' What is AI Ethics?'
hidden: false
---

<text-box variant='Intro' name='Learning objectives'>

- VALUES, NORMS AND FACTS
- DO GOOD, NOT BAD

</text-box>

### Introduction

In the previous section, we discussed on the nature of ethics. <br>
In this section, we focus on ethical concepts, such as values, norms and normativity.

# WHAT IS GOOD: VALUES, NORMS AND FACTS

## Values

When we say things like “This AI application is more fair than this other, and it promotes non-
discriminatory practices” we appeal to values. “Values” provide standards and ideals with which
evaluate things, people, choices, actions, and events. As a concept, “value” means, roughly, the
degree of importance of a thing or an action.

When we evaluate the value of a thing, we perform a “value judgment”. A “value judgment” is any
judgment that can be expressed in the form "X is good, worthy or desirable" or "X is bad, worthless,
or undesirable."

Values can be divided into several subtypes: economical, aesthetic, epistemic, prudential, or
religious values. In this course, we are primarily interested in moral values. However, the other
types of values can be relevant to moral considerations as well. For example, sometimes economical
values can play morally significant role. For instance, when we evaluate the impact of our choices
in terms of injuring or harming another person, harms can be given an economic interpretation.

Intrinsic and extrinsic values

Philosophers make a distinction between intrinsic and extrinsic, or instrumental values. For
example, if money is good, it has instrumental value. It is valuable only because one can use it for
other things. For example, if one has money, one may use it to buy a good computer or a quick
internet. These things, in turn, may be good for what they lead to: for example, exciting discussions
in a social media. And those things, in turn, may be good only for what they lead to – for example a
friendship, or well-being. The point is that money derive its goodness from other things, and hence,
it has only instrumental value.

If the instrumental value is dependent on something else, eventually something must be intrinsically
valuable i.e. valuable in its own rights. It explains the goodness to be found in all the other things.
Intrinsically valuable things are typically “big ethical topics”, such as the well-being, happiness, or
freedom. These things are good as they are (cf. Aristotle, Nicomachean Ethics, 1094a).

## NORMS

“Normativity” refers to an evaluative standard, “a norm”, by which we designate actions, behavior
or outcomes as desirable or permissible (and others as undesirable or impermissible). Importantly,
norms do not describe how the world is, as descriptive statements do. Instead, they prescribe how
the world should be. This is, they imply “ought-to” evaluations, in distinction to sentences that
provide "is" types of assertions. For instance, a sentence "Some machine-learning systems are
black-box systems" is descriptive, while a sentence “Machine-learning systems should be
transparent" is normative.

In practice it is often difficult to know, whether a sentence is normative or descriptive. For example,
a sentence "all deep learning neural network systems have more than three layers" can be taken as
descriptive claim on the structure of DNN systems. Alternatively "All DNN- systems have more
than three layers” can also be taken as a norm, which refers to a (normative) definition of DNN-
systems. In this case, the norm is used to evaluate, whether an entity can be counted as an instance
of DNN-system or not.

Norms do not purport to describe anything, but to prescribe, create or change something. Whereas
the truth of a descriptive statement is based on its correspondence to reality, following Aristotle
some philosophers think that the truth of normative statements are based on their correspondence to
right desire. Other philosophers, such as John Searle, argue that norms are neither true or false, but
only successful or unsuccessful.

Normativity, and norms, are important not only for ethics, but these concepts play role in many
other fields too. Laws, political decisions, standardizations and many other such things are
thoroughly normative. According to some philosophers - e.g. Saul Kripke - rules (including
mathematical and algorithmic rules) are normative too; they do not only describe, but they also
constrain and restrict.

Of course, the normativity of formal rules, political goals or laws does not imply that they are
normative in moral sense. For example, legal norms (“laws”) and moral norms are not synonymes.
The fact that “X is a law” does not make X as morally good. Instead, one can always ask: “Is this
law good or bad?”. This question alone suffices to illustrate, how laws and morality refer to
different things.

### Exercise 2

## FACTS

Hume´s guillotine, as articulated by the Scottish philosopher David Hume (1711–76), states that we
should not make claims about what should be, based only on statements about what is. Of course,
facts – and knowledge about the facts – are important for normative and moral views. Hume´s
remark is simply that moral preferences cannot be justified only by appealing to facts. It is one thing
to talk about facts, and another thing how we judge or wish them to be.

As the famous slogan goes, one cannot derive the “ought from is”. For example, the fact that there
is biased data does not imply that the data should be biased. Instead, whether the data should be
biased depends on our normative, practical and ethical preferences, not only on the facts in the
world. However, not all philosophers agree with this claim, and there are some counterarguments on this issue. (For criticisms of the fact-value dichotomy, see Putnam, Hilary (2003). The Collapse
of the Fact-Value Dichotomy and Other Essays, Harvard University Press, pp.21-22.)

<text-box variant="hint" name="INFOBOX">

D. Hume: A Treatise of Human Nature (1739):

“ In every system of morality, which I have hitherto met with, I have always remarked, that the
author proceeds for some time in the ordinary way of reasoning, and establishes the being of a God,
or makes observations concerning human affairs; when of a sudden I am surprised to find, that
instead of the usual copulations of propositions, is, and is not, I meet with no proposition that is not
connected with an ought, or an ought not. This change is imperceptible; but is, however, of the last
consequence. For as this ought, or ought not, expresses some new relation or affirmation, 'tis
necessary that it should be observed and explained; and at the same time that a reason should be
given, for what seems altogether inconceivable, how this new relation can be a deduction from
others, which are entirely different from it. But as authors do not commonly use this precaution, I
shall presume to recommend it to the readers; and am persuaded, that this small attention would
subvert all the vulgar systems of morality, and let us see, that the distinction of vice and virtue is not
founded merely on the relations of objects, nor is perceived by reason.”

</text-box>

## DO GOOD, NO BAD

### Harms

When we evaluate the morally meaningful factors of actions, consequences – such as harms and
benefits – matter. To make ethically sustainable decisions, we should be able to justify our decisions by appealing to the comparisons of consequences.

In practice, it is often difficult to compare the real harms or real benefits of real actions in advance. First, harms and benefits are subject-dependent, and people have different views of them. In so far we evaluate real consequences for real people, consequences are always more or less subjective, and influenced by value commitments, practical circumstances and by personal factors. For example, is it worse to increase one's chance of fatal car accident by 25% by driving a car that is easy to use and cheap, or to drive a car, which is safer, but difficult to use, slow and expensive?

Obviously, the answer depends on subjective preferences and domain-related factors. In MIT´s famous “Moral Machine”- experiment the comparison was made in terms of preferences. The MM experiment is designed to measure the preferred outcomes in various schenarios. However, the measure of degree of (subjective) preference is not a measure of magnitude of harm or benefit, and thus, by asking only preferences one cannot justify decisions on real consequences.

Also, a specific harm or a specific benefit may have different implications in different circumstances. For example, whether or not the faster car will be more beneficial depends on the intended use of it – if it is intended to be a school bus, then we should prioritize the safety, but if it is used as a racing car, then the answer may be different.

In ethics, practical and subjective factors are sometimes simply idealized away. If the type of harm or benefit is held constant, and subjective factors are simply ignored, then it is possible to compare
actions in terms of possible risks (Kauppinen, in progress). Instead of focusing on real consequences, one focuses on comparing the likelihood of producing for a particular consequence in idealized situations. However, this changes the basis of comparison: It is more accurate to speak of "risk-benefit" comparisons rather than cost-benefit -comparisons.

### RISKS

Generally, "risk" is commonly used to mean a likelihood of a danger or a hazard that arises unpredictably, or in a more technical sense, the probability of some resulting degree of harm. This formal notion requires that one can treat the resulting harms in some exact quantities (i.e., not only quantify them so as to be able to rank order them, but assign the harms quantities that may be added
and subtracted, see Kauppinen, under review for discussion). If the consequences – risks, possible harms and benefits - can be quantified, there are formal mathematical, computational and
algorithmic techniques which may be helpful in clarifying the tradeoffs involved in exchanging one harm or benefit for another.
In practice it is really difficult to estimate the probabilities for this kind of benefit-risk- evaluations. For example, let´s compare the possible benefits or risks of military robotics. Contemporary
military applications are largely remotely operated or semi-autonomous. However, over time these machines are likely to become eventually fully autonomous. Many claim that the benefits of this technology are that it reduces civilian and military casualties,
and it helps follow International Humanitarian Law and other legal and ethical codes of conduct in war (see Lin et al. 2008, 2013; Sullins 2009b). However, it is not clear, whether the risk to civilians will be automatically reduced, if we accepted automated warfare. On the contrary, statistically in the first decades of war in the 21st century robotic weaponry has been involved in numerous killings of both soldiers and noncombatants. Moreover, the possibility
to use various techniques – such as adversial patches etc. – to fool and manipulate the automated weapons increase some specific risks. The overall level of risks is also dependent on the ease in
which wars might be declared, if robots will be taking most of the physical risk (Asaro 2008; Sharkey 2011, for discussion see also Vincent C. Müller – SEP, ‘Ethics of AI & Robotics’ – vs.
0.84 – public draft, http://www.sophia.de -18 March 2019 11).
More importantly, as Kauppinen correctly remarks (under review), these risk-benefit calculations sometimes obscure the morally significant matters, such as the fairness of the calculations (see
CHAPTERS 6 and 7 for more details.) Risk-benefit comparisons alone do not suffice to answer the question of moral value of our choices, even when they may help to compare the consequences of actions.

### EXERCISE 3

Various empirical studies have documented the differences in value aspects of cultures around the world and explicated their sources and implications (e.g., Hofstede, 1982, 1991; Schwartz, 1997; Smith & Schwartz, 1997, Schwartz & 2001). These studies reveal a great deal of variation in the value priorities of individuals within societies as well as groups across nations. Yet, a striking degree of consensus across individuals and societies on values have also been documented. In their classical study, Scwartz and colleagues suggested that certain values are especially important (e.g., honesty and other prosocial values) and others are much less important (e.g., wealth and other power values). They also found that there are some values for which consensus regarding their importance is low (e.g., pleasure and other hedonism values).
