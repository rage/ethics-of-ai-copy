---
path: '/chapter-2/2-common-good_calculating-consequencies'
title: 'Common good: calculating consequencies'
hidden: false
---

<text-box name="Example">

Suppose you are the Chief Digital Officer in Helsinki City. You are asked to consider whether the city’s health care organisation should move from “reactive” healthcare to “preventive” care. You read a report. It tells about novel, sophisticated machine learning systems. Systems would help health authorities to forecast the possible health risks of citizens.

These methods produce predictions by combining and analysing various sources of medical and health care systems. By analysing a large number of criteria data, high risk individuals could be identified and prioritized. These high risk individuals could proactively be invited to doctor’s appointments to get proper treatment.

The report mentions many advantages. For example, sickness prevention has  a lot of potential to improve the health and quality of life for citizens. Further, it would allow better impact estimation and planning of basic health care services. Preventive healthcare has also potential to reduce significantly social and health care costs. These savings, the report emphasizes, could be used for common good.

The report, however, also includes some concerns.  For example, the systems raise a number legal and ethical issues regarding privacy, security, and the use of data. The raport asks, for example, where is the border between acceptable prevention and non-acceptable intrusion? Has the city a right to use private, sensitive medical data for individuating high risk patients? How are the consents to be given, and what will happen to people, who don´t give their consent? What about those people, who do not give consents, because they are not able to?

On the other hand, the report also raises the fundamental question of the city’s role:  if the city has information about the potential health risk and does not act upon the data, is the city guilty of neglect?  Are citizens treated equally in physical and digital worlds? If in real life, a person passes out, we call an ambulance without having explicit permission to do so. In the digital world, privacy concerns may prevent contacting citizens.

</text-box>

What do you think about the above example? Would you as a chief digital officer promote the use of preventative methods? If you think “yes, the city should seek an ethically and legally acceptable way to use those methods. They just have so many advantages compared to possible risks”, you were probably using a form of moral reasoning called "utilitarianism".

<text-box name="Utilitarianism">

**Utilitarianism** is is a family of ethical  theories. It conceives “benefits” as actions that maximize well-being across all affected individuals. It is a version of consequentialism, which states that the consequences of any action are the only standards of right and wrong.

</text-box>

According to utilitarianists,  morally right actions are the ones that produce the greatest balance of benefits over harms for everyone affected. Unlike other, more individualistic forms of consequentialism (such as egoism) or unevenly weighted consequentialism (such as prioritarianism), utilitarianism considers the interests of all humans equally. However, utilitarianists disagree on many specific questions, such as whether actions should be chosen based on their likely results (act utilitarianism), or whether agents should conform to rules that maximize utility (rule utilitarianism). There is also disagreement as to whether total (total utilitarianism), average (average utilitarianism) or minimum utility should be maximized.

For utilitarianists, utility – or benefit – is defined in terms of well-being or happiness. For instance, Jeremy Bentham, the father of utilitarianism, characterized utility as "that property… (that) tends to produce benefit, advantage, pleasure, good, or happiness…(or) to prevent the happening of mischief, pain, evil, or unhappiness to the party whose interest is considered."

Utilitarianism offers a relatively simple method for deciding, whether an action is morally right or not. To discover what we should do, we first identify the various actions that we could perform. Second, we estimate the benefits and harms that would result from each action. And third, we choose the action that provides the greatest benefits after the costs have been taken into account.


**PICTURE: PAST, PRESENT, FUTURE**

Utilitarianism provides many interesting ideas and concepts. For example, the principle of “diminishing marginal utility” is fruitful for many purposes. According to it, the utility of an item decreases as the supply of units increases (and vice versa). For example, when you start to work out, at first you benefit greatly. Your results get better dramatically, but the longer you continue working out, each individual training session has a smaller impact. If you work out too often, the utility diminishes, and you’ll end up having issues, and you’ll start to suffer from the symptoms of overtraining. Or, if you eat one candy, you´ll  get a lot of pleasure, but if you eat too much candy, you may end up having diabetes. This paradox of benefits should always be remembered, when we evaluate the consequences of actions. What is common good now, may not be common good in future.

**PICTURE of diminishing marginal utility**

## The problems with utilitarianism

Utilitarianism is not a perfect account on moral decision making. It has been critized on many grounds. For
example, utilitarian calculation requires that we assign values to the benefits and harms resulting from our
actions and compare them with the consequencies that might result from other actions. But it's often
difficult, if not impossible, to measure and compare the values of all relevant benefits and costs in
advance.

<text-box variant="hint" name="Risks">

"Risk" is commonly used to mean a likelihood of a danger or a hazard that arises unpredictably, or in a more technical sense, the probability of some resulting degree of harm. In AI ethics, harms and risks are taken to arise from design, inappropriate application or intentional misuse of the technology. Typical examples are risks such as discrimination, violation of privacy, security issues, cyberwarfare or malicious hacking. In practice, it is difficult to compare the risks or benefits.

**Firstly**, they are influenced by value commitments, subjective and diverse preferences, practical circumstances and personal and cultural factors.

**Secondly**, harms or benefits are not static. The marginal utility of an item diminishes in a way that can be difficult to foresee. Moreover, a specific harm or a specific benefit may have different utility value in different circumstances. For example, whether or not the faster car will be more beneficial depends on the intended use of it – if it is intended to be a school bus, then we should prioritize the safety, but if it is used as a racing car, then the answer may be different.

**Thirdly**, real world situations are typically so complex that it is difficult to foresee or compare all the risks and benefits in advance. For example, let´s analyze the possible consequences of military robotics. Although  contemporary military robots are largely remotely operated or semi-autonomous, over time they are likely to become fully autonomous. According to some estimates, robots reduce civilian and military casualties.  According to other estimates, they do not reduce the risk to civilians. Statistically, in the first decades of war in the 21st century, robotic weaponry has been involved in numerous killings of both soldiers and noncombatants. The possibility to use various techniques – such as adversial patches etc. – to fool and manipulate the automated weapons complicates the comparison by increasing specific risks of causing harm to civilians. The overall level of risks is also dependent on the ease in which wars might be declared, if robots are taking most of the physical risk.

**Fourthly**, utilitarianism fails to take into account other moral aspects. It is easy to imagine situations, where developed technology would produce great benefits for societies, but their use still would raise important ethical questions. For example, let’s think about the case of a preventive health care system. Indeed, the system may indeed be beneficial for many. And still, it forces us to ask, whether fundamental human rights, such as privacy, matter. Or, what happens to the citizen’s right to not-to-know about the possible health problems? Many of us would want to know, if they belong to high risk citizens, but what if one does not? Can a city force to know? Or, how can we make it sure that everyone can have an equal access to the possible benefits of a preventative system?

</text-box>

### Summary

Despite these problems, the principles of utilitarianism may help to consider the immediate and the less immediate consequences of our actions. One just should remember that in real life, defining “common good” requires a diversity of viewpoints.

<text-box name="Common good">

Common good prerequisites that all should have access to the benefits of AI. This highlights the importance of ensuring that potential benefits of AI do not accumulate unequally, and are made accessible to as many people as possible. And, AI should be aligned with values, goals, and norms, respecting cultural and individual diversity to a sufficient degree.

Common good is not a singular, but a plural. Identifying social and moral norms of the specific community, in which an AI will be deployed is, thus, obligatory. It is the only way to bring AI´s potentially significant and diverse benefits to society and facilitate, among other things, greater wellbeing and welfare for all.

</text-box>

<text-box name="Nozick’s utility monster">

One of the biggest difficulties with utilitarianism is the question of utility: what is it really? Technically utility is only a measure, a numeric quantity, that describes some kind of underlying “good” which we want to maximize. Say, pleasure, or well being (which hedonist philosophers would claim to be the same thing). Pleasure is at least to some extent a subjective experience, and utility, as a measure, should transform it into an intersubjectively comparable number. That is a high bar to reach.

Assuming such a measure as utility does in fact exist, philosopher Robert Nozick presents the following puzzle. There is a creature called the Utility Monster. Their hedonistic mind is wired so that, given any resource, they will receive more pleasure from it than any other individual would. They simply enjoy apples, cars, coffee, freedom etc. more than anybody else does.  This means they gain more utility from them, and if we are morally obligated to maximise the utility produced by the resources we have, the conclusion is clear: everything we have to the Utility Monster. Nothing to anybody else.

Does this make utilitarianism unpalatable? Is there a way for the utilitarian to argue that the puzzle Nozick posed is not really a problem?

</text-box>

### Exercise

1. So, if you´d be the Chief Digital Officer, you´d have to write a report on the possible benefits and
risks of the novel health care system. To do so, you are asked to:

(i) evaluate the immediate, here-and-now benefits, and possible risks

(ii) evaluate the medium-term impacts, such as the positive and negative impacts on the medical
care system, the job markets and so on.

(iii) evaluate the long-term goals, such as consider the positive and negative ethical impacts in
terms of “common good”
