---
path: '/chapter-2/2-do-good-no-bad'
title: 'Do good no bad'
hidden: false
---

## Do Good, No Bad

### Harms

When we evaluate the morally meaningful factors of actions, consequences – such as harms and
benefits – matter. To make ethically sustainable decisions, we should be able to justify our decisions by appealing to the comparisons of consequences.

In practice, it is often difficult to compare the real harms or real benefits of real actions in advance. First, harms and benefits are subject-dependent, and people have different views of them. In so far we evaluate real consequences for real people, consequences are always more or less subjective, and influenced by value commitments, practical circumstances and by personal factors. For example, is it worse to increase one's chance of fatal car accident by 25% by driving a car that is easy to use and cheap, or to drive a car, which is safer, but difficult to use, slow and expensive?

Obviously, the answer depends on subjective preferences and domain-related factors. In MIT´s famous “Moral Machine”- experiment the comparison was made in terms of preferences. The MM experiment is designed to measure the preferred outcomes in various schenarios. However, the measure of degree of (subjective) preference is not a measure of magnitude of harm or benefit, and thus, by asking only preferences one cannot justify decisions on real consequences.

Also, a specific harm or a specific benefit may have different implications in different circumstances. For example, whether or not the faster car will be more beneficial depends on the intended use of it – if it is intended to be a school bus, then we should prioritize the safety, but if it is used as a racing car, then the answer may be different.

In ethics, practical and subjective factors are sometimes simply idealized away. If the type of harm or benefit is held constant, and subjective factors are simply ignored, then it is possible to compare
actions in terms of possible risks (Kauppinen, in progress). Instead of focusing on real consequences, one focuses on comparing the likelihood of producing for a particular consequence in idealized situations. However, this changes the basis of comparison: It is more accurate to speak of "risk-benefit" comparisons rather than cost-benefit -comparisons.

### Risks

Generally, "risk" is commonly used to mean a likelihood of a danger or a hazard that arises unpredictably, or in a more technical sense, the probability of some resulting degree of harm. This formal notion requires that one can treat the resulting harms in some exact quantities (i.e., not only quantify them so as to be able to rank order them, but assign the harms quantities that may be added
and subtracted, see Kauppinen, under review for discussion). If the consequences – risks, possible harms and benefits - can be quantified, there are formal mathematical, computational and
algorithmic techniques which may be helpful in clarifying the tradeoffs involved in exchanging one harm or benefit for another.
In practice it is really difficult to estimate the probabilities for this kind of benefit-risk- evaluations. For example, let´s compare the possible benefits or risks of military robotics. Contemporary
military applications are largely remotely operated or semi-autonomous. However, over time these machines are likely to become eventually fully autonomous. Many claim that the benefits of this technology are that it reduces civilian and military casualties,
and it helps follow International Humanitarian Law and other legal and ethical codes of conduct in war (see Lin et al. 2008, 2013; Sullins 2009b). However, it is not clear, whether the risk to civilians will be automatically reduced, if we accepted automated warfare. On the contrary, statistically in the first decades of war in the 21st century robotic weaponry has been involved in numerous killings of both soldiers and noncombatants. Moreover, the possibility
to use various techniques – such as adversial patches etc. – to fool and manipulate the automated weapons increase some specific risks. The overall level of risks is also dependent on the ease in
which wars might be declared, if robots will be taking most of the physical risk (Asaro 2008; Sharkey 2011, for discussion see also Vincent C. Müller – SEP, ‘Ethics of AI & Robotics’ – vs.
0.84 – public draft, http://www.sophia.de -18 March 2019 11).
More importantly, as Kauppinen correctly remarks (under review), these risk-benefit calculations sometimes obscure the morally significant matters, such as the fairness of the calculations (see
CHAPTERS 6 and 7 for more details.) Risk-benefit comparisons alone do not suffice to answer the question of moral value of our choices, even when they may help to compare the consequences of actions.

### Exercise 2

Various empirical studies have documented the differences in value aspects of cultures around the world and explicated their sources and implications (e.g., Hofstede, 1982, 1991; Schwartz, 1997; Smith & Schwartz, 1997, Schwartz & 2001). These studies reveal a great deal of variation in the value priorities of individuals within societies as well as groups across nations. Yet, a striking degree of consensus across individuals and societies on values have also been documented. In their classical study, Scwartz and colleagues suggested that certain values are especially important (e.g., honesty and other prosocial values) and others are much less important (e.g., wealth and other power values). They also found that there are some values for which consensus regarding their importance is low (e.g., pleasure and other hedonism values).

Please, evaluate the following values by giving the stars (modified from Schwartz & 2001)

Social status and prestige

Control or dominance over people and resources

Personal success through demonstrating competence according to social standards

Pleasure and sensuous gratification for oneself

Excitement, novelty, and challenge in life

Individuality

Independence of thought and action-choosing

Creativity

Equality of all men

Protection for the welfare of all people

Protection for nature

Preservation and enhancement of the welfare of people with whom one is in frequent personal contact

Honesty

Loyalty

Respect of other people

Commitment and acceptance of the customs and ideas that traditional

Culture or religion provide the self

Restraint of actions,

Avoidance of violate social expectations or norms

Politeness

Self-discipline

Honoring parents and elders

Safety, harmony and stability of society,

Safety and stability of relationships

Security

National security

Social order

Cleaness

### Execise 3

Please, read the following text. (Source). Do you find any values from the text? Please, mark the
expressions that refers to values by clicking them.

1. **The EU’s Rights’ Based Approach to AI Ethics**

The High-Level Expert Group on AI (“AI HLEG”) believes in an approach to AI ethics that uses
the fundamental rights commitment of the EU Treaties and Charter of Fundamental Rights as the
stepping stone to identify abstract ethical principles, and to specify how concrete ethical values can
be operationalised in the context of AI. The EU is based on a constitutional commitment to protect
the fundamental and indivisible rights of human beings, ensure respect for rule of law, foster
democratic freedom and promote the common good. Other legal instruments further specify this
commitment, like the European Social Charter or specific legislative acts like the General Data
Protection Regulation (GDPR). Fundamental rights cannot only inspire new and specific regulatory
instruments, they can also guide the rationale for AI systems’ development, use and
implementation – hence being dynamic.

The EU Treaties and the Charter prescribe the rights that apply when implementing EU law; which
fall under the following chapters in the Charter: dignity, freedoms, equality and solidarity, citizens’
rights and justice. The common thread to all of them is that in the EU a human-centric approach is
upheld, whereby the human being enjoys a unique status of primacy in the civil, political, economic
and social fields.

The field of ethics is also aimed at protecting individual rights and freedoms, while maximizing
wellbeing and the common good. Ethical insights help us in understanding how technologies may
give rise to different fundamental rights considerations in the development and application of AI,
as well as finer grained guidance on what we should do with technology for the common good
rather than what we (currently) can do with technology. A commitment to fundamental rights in the
context of AI therefore requires an account of the ethical principles to be protected. In that vein,
ethics is the foundation for, as well as a complement to, fundamental rights endorsed by humans.

The AI HLEG considers that a rights-based approach to AI ethics brings the additional benefit of
limiting regulatory uncertainty. Building on the basis of decades of consensual application of
fundamental rights in the EU provides clarity, readability and prospectivity for users, investors and
innovators.

2. **From Fundamental rights to Principles and Values**

To give an example of the relationship between fundamental rights, principles, and values let us
consider the fundamental right conceptualised as ‘respect for human dignity’. This right involves
recognition of the inherent value of humans (i.e. a human being does not need to look a certain way,
have a certain job, or live in a certain country to be valuable, we are all valuable by virtue of being
human). This leads to the ethical principle of autonomy which prescribes that individuals are free
to make choices about their own lives, be it about their physical, emotional or mental wellbeing
(i.e. since humans are valuable, they should be free to make choices about their own lives). In turn,
informed consent is a value needed to operationalise the principle of autonomy in practice.
Informed consent requires that individuals are given enough information to make an educated
decision as to whether or not they will develop, use, or invest in an AI system at experimental or
commercial stages (i.e. by ensuring that people are given the opportunity to consent to products or
services, they can make choices about their lives and thus their value as humans is protected).

3. **Normativity, norms and standardization**

Normativity plays a significant role in standardization. Because of this, for example in International
Organization for Standardization (ISO) Directives Part 2 normativity is characterized in terms of
"elements that describe the scope of the document, and which set out provisions". Provisions
include "requirements", "recommendations" and "statements". "Statements" include permissions,
possibilities and capabilities. A "requirement" is an "expression in the content of a document
conveying criteria to be fulfilled if compliance with the document is to be claimed and from which
no deviation is permitted." These expressions – usually denoted by the verbal forms of “shall” – are
typical examples of the explicit role of normativity in standardization.

Please, read the following text on ISO 26262 standard, and mark all the normative expressions by
clicking them with the mouse.

(Source: Salay, R. & Czarnecki, K. (2018). Using Machine Learning Safely in Automotive
Software: An Assessment and Adaption of Software Process Requirements in ISO 26262
https://arxiv.org/pdf/1808.01614)

ISO Subsection 5) Initiation of product development at the software level

5.1 Process requirements

5.1.1 ML development decision gate

Due to the low level of maturity of ML-based components and obstacles O1 and O2, we propose

a new requirement to assess the necessity of using ML to implement safety requirements. This is
a “decision gate” for moving ahead with development using ML. The key criterion here is the
specifiability of the safety requirement. If it can be completely specified, a traditional
programming approach to implementing the requirement is possible and should be taken. Thus,
we add the following process requirement.

(Req MLIN1) An assessment shall be performed to determine whether the safety requirement
must be implemented by an ML-component or can acceptably be implemented using a
programmed component. If the latter case holds then programming shall be used rather than
ML.

… For example, if the safety requirement is to “detect all pedestrians within 10 meters”, this is not
completely specifiable because it is unclear what the complete set of necessary and sufficient
conditions are for identifying what a pedestrian is. On the other hand, the safety requirement
“detect obstacles within 10 meters” could be precisely specified and implemented using
programming because obstacle detection can be performed with an appropriate combination of
sensors (e.g., LIDAR, RADAR, etc.) and signal processing.

Different strategies are possible for “containing” safety requirements that are not completely
specifiable. If a component has multiple safety requirements and some are completely
specifiable, then consideration should be given to splitting the component into a programmed
part and an ML part. For example, if the safety requirement for pedestrian detection and the one
for obstacle detection is assigned to the same component, the latter requirement could be split off
and implemented using programming.

In some cases, it may be possible to strengthen a safety requirement to make it completely
specifiable if this more conservative requirement still provides acceptable functionality. For
example, the obstacle detection requirement above is stronger than pedestrian detection since
detecting all obstacles includes detecting all pedestrians. If the requirement is needed for
supporting pedestrian avoidance then the more conservative one is sufficient. However, if it is
needed for deciding which obstacle to hit when hitting one of two obstacles is unavoidable, then
it is essential to know which of the obstacles is a pedestrian.

Further Reading: von Wright, G. H. (1963), Norm and Action: a Logical Enquiry, Routledge &
Kegan Paul, London.
