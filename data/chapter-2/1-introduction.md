---
path: '/chapter-2/1-introduction'
title: 'Introduction'
hidden: false
---

“AI inevitably becomes entangled in the ethical and political dimensions of vocations and practices in which
it is embedded. AI Ethics is effectively a microcosm of the political and ethical challenges faced in society.”
- Brent Mittelstadt

The principle of “beneficence” says “Do good”, while the principle of non-maleficence states “Do no harm”.
Although these two principles may look synonymous, they represent distinct principles. The former
encourages the creation of beneficial AI (“AI should be developed for the common good and the benefit of
humanity”), while the latter concerns the negative consequences and risks of AI.

Generally, AI ethics have been primarily concerned with the principle of “non-maleficence”. Discussion has
focused mostly on questions of how developers, manufacturers, authorities or other stakeholders should
minimise the ethical risks - discrimination, privacy protection, physical and social harms - that can arise
from AI applications. Often, they are in terms of intentional misuse, malicious hacking, technical measures
or risk-management strategies.

Critics claim that this emphasis makes ethics a matter of finding technical solutions for technical problems.
Moral problems are seen as things that can be solved by technical “fixes”, or by good design alone. The
wider ethical and societal context in which technical systems are embedded is forgotten. Many significant
issues that direct the control, governance and societal dimensions of AI are ignored. Technology researcher
Evgeny Morozov calls this “tech solutionism”: The conviction that problems caused by technology can
always be fixed by more technology.

As a result, deep and difficult ethical problems are oversimplified and unanswered. One of this questions is
the problem of “common good”: What, exactly, is it?
In this chapter, we´ll take a look at one classical philosophical answer.
