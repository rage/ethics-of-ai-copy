---
path: '/chapter-4/3-transparency-and-the-risks-of-openness'
title: 'Transparency and the risks of openness'
hidden: false
---

<hero-icon heroIcon='chap4'/>

<styled-text>

Transparency often denotes a modern, ethico-socio-legal “ideal” (Koivisto 2016), a normative demand for the acceptable use of technology in our societies. It is a reflection of the ideal of “openness”, that is framed in terms of “open government”, “open data”, “open source/code/access”, as well as “open science” (Larsson 2020). In this way, transparency considerations are needed to mitigate the equal distribution of scientific advancements so that the benefits of AI development can be made accessible for all people.

</styled-text>

<text-box>

Paradoxically, the ideal of openness can lean to harmful consequences, too. For example, the transparency of social media platforms has led to several instances of misuse and democratic challenges. Transparency can create security risks. Too much transparency may lead to leaking of privacy-sensitive data into the wrong hands. Or the more that is revealed about the algorithms and the data, the more harm a malicious actor can cause. Algorithms can be hacked, and information may make AI more vulnerable to intentional attacks. Entire algorithms can also be stolen based simply on their explanations alone.

</text-box>

<styled-text>

In summary, while there is a need to develop more transparent practices for AI, there is also a need to  develop practices that can help us to avoid abuse. While transparency may help to mitigate ethical issues – such as fairness or accountability – it also creates ethically important risks. Too much openness in the wrong context may defeat the positive development of AI-enabled processes. Taken together, it is clear that the ideal of full transparency of algorithms should be carefully considered, and we will have to find a balance between security and transparency considerations.

</styled-text>

<quiz id="ed8ccfc3-72e5-4596-afa0-4ff5b69dfd59"> </quiz>
