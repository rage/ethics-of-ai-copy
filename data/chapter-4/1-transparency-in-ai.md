---
path: '/chapter-4/1-transparency-in-ai'
title: 'Transparency in AI'
hidden: false
---

<hero-icon heroIcon='chap4'/>

## The principle of transparency

<p style="color:red;">Let’s examine an example that shows why transparency in AI is important, and what major issues are affected by it.</p>

<text-box>

Imagine a facial recognition system called MYFACE. MYFACE is used for security purposes in the airport. Usually it works perfectly, but one day it starts to miscategorize individuals as potentially dangerous. As a result, several innocent people are arrested. Would it be important to know why the system made all these mistakes? Should we be able to explain why it made mistakes? And why would this matter?

-- picture --

</text-box>

<styled-text>

Some contemporary machine learning systems are so-called “black box” systems, meaning we can’t really see how they work.  This “opacity”, or lack of visibility, can be a problem if  we use these systems to make decisions that have an effect on individuals.

Individuals have a right to know how critical decisions – such as who gets accepted for a loan application, who gets paroled, and who gets hired – are made. This has led many to call for “more transparent AI”.

### I. Transparency in AI


Transparency is a property of a system that makes it possible to get certain information regarding a system’s inner workings. But what information that is, and whether it is ethically relevant, depends largely on the ethical issue we are trying to answer. Transparency itself is ethically neutral and is not an ethical concept. Instead, it constitutes an ideal. Transparency is something that can manifest in many different ways, and something that can present a solution for underlying ethical questions. In this sense,  transparency is relevant at least to the three following issues:

**1) The justification of decisions.** Good governance in public or private sectors involves non-arbitrariness of decisions. This is applied to any kind of decision-making that has an ethically or legally    relevant effect on individuals. Non-arbitrariness means access to justifications about “why was this decision reached, and on what grounds?” Furthermore, especially in the case of public governance, the capacity to   contest and appeal are crucial. This represents a demand to right wrongs.

**2) A right to know. According to human rights**, people are entitled to have explanations on how decisions were made so that they can maintain genuine agency, freedom and privacy (for more on human rights, see chapter 5). Freedom entails the right to get answers to questions such as “How am I being tracked? What kind of inferences are being made about me? And how, exactly, have the inferences about me been made?”

**3) A moral obligation to understand the consequences of our actions**. As a community, we also have a responsibility for managing risks. There is a moral obligation, up to some reasonable level, to understand and predict the consequences of the kinds of technologies one brings into the world. That is, saying “we can’t understand now what it will do” is not a valid argument for unleashing a system that causes harm. Instead, it is our moral duty to explore the possible risks.

These three points can all be summarized as calls for sufficient information. Do we know whether and to what extent this algorithmic decision is justified? Do I know how inferences about me are made? To what extent I am responsible for the actions of the system, and how much I should know about the inner workings of the system to be able to take that responsibility?


### V) Transparency and the risks of openness

Transparency often denotes a modern, ethico-socio-legal “ideal” (Koivisto 2016), a normative demand for the acceptable use of technology in our societies. It is a reflection of the ideal of “openness”, that is framed in terms of “open government”, “open data”, “open source/code/access”, as well as “open science” (Larsson 2020). In this way, transparency considerations are needed to mitigate the equal distribution of scientific advancements so that the benefits of AI development can be made accessible for all people.

</styled-text>

<text-box>

Paradoxically, the ideal of openness can lean to harmful consequences, too. For example, the transparency of social media platforms has led to several instances of misuse and democratic challenges. Transparency can create security risks. Too much transparency may lead to leaking of privacy-sensitive data into the wrong hands. Or the more that is revealed about the algorithms and the data, the more harm a malicious actor can cause. Algorithms can be hacked, and information may make AI more vulnerable to intentional attacks. Entire algorithms can also be stolen based simply on their explanations alone.

</text-box>

<styled-text>

In summary, while there is a need to develop more transparent practices for AI, there is also a need to  develop practices that can help us to avoid abuse. While transparency may help to mitigate ethical issues – such as fairness or accountability – it also creates ethically important risks. Too much openness in the wrong context may defeat the positive development of AI-enabled processes. Taken together, it is clear that the ideal of full transparency of algorithms should be carefully considered, and we will have to find a balance between security and transparency considerations.

</styled-text>

<quiz id="f94f13d3-3983-4d86-811c-881e1282c275">

There is a need to translate algorithmic concepts into everyday language. Most people without a  background in computer science are not familiar with the basic vocabulary of AI. This has a direct impact on their ability to understand recent developments.

<img src="./dog_agent.png" alt="dog agent" style="width: 50px; height: 50px">


<img src="./robot_agent.png" alt="robot agent">


<img src="./technical_agent.png" alt="technical agent" style="width: 50%">

<br>

Compare these three visualizations of reinforcement learning algorithms. Which one of them is the most understandable? Why?

<br>

</quiz>
