---
path: '/fi/chapter-3/4-the-problem-of-individuating-responsibilities'
title: 'Vastuiden erottelun ongelma'
hidden: false
---

<hero-icon heroIcon='chap3'/>


<styled-text>

Vastuullisuutta pidetään usein yksilön tai organisaation oikeudellisena ja eettisenä velvoitteena ottaa vastuu tekoälyjärjestelmien käytöstä ja julkistaa tulokset avoimesti. Tämä muotoilu edellyttää ”voimasuhdetta”. Sillä yksilöidään, kuka on vastuussa ja ketä on syytettävä.

Tunnetusti on kuitenkin osoittautunut vaikeaksi asettaa erityisiä kriteerejä sille, miten vastuut tarkalleen ottaen olisi jaettava, kohdistettava ja määriteltävä. Näistä kysymyksistä käydään keskusteluja monissa maissa. Kansainväliset toimijat, kuten Euroopan unioni ja G7-maat, ovat käsitelleet näitä kysymyksiä ratkaisemattomina haasteina.

Miksi on niin vaikeaa asettaa kriteerejä sille, kuka on vastuussa?

* **Ensinnäkin** vastuiden laatu vaihtelee. Toimija on vastuussa tietystä teosta tai tekemättäjätöstä, mutta vastuun laatu riippuu sidosryhmästä. Vaikka valitsemalla tietyn teon voit saada vastuun, vastuun laatu riippuu myös ominaisuuksistasi. Älykkäät teknologiat monimutkaistavat tätä kysymystä entisestään.

    Kun annamme yhä enemmän päätöksentekotehtäviä ja -toimintoja algoritmeille, muokkaamme myös päätöksentekorakenteita. Tekoäly lisää älykkyyttämme antamalla meille enemmän laskentatehoa, mahdollistamalla paremmat ennusteet ja parantamalla anturilaitteitamme. Ihmisistä ja koneista tulee kognitiivisia hybridejä. Ihmiset ja koneet tekevät kognitiivista (ajattelu) ja episteemistä (tieto) yhteistyötä sekä yksilötasolla että kollektiivisella tasolla. Näin syntyy systeemisiä ominaisuuksia.

    Usein ajatellaan riittävän, että ihminen osallistuu prosessiin (human-in-the-loop) tai voi osallistua siihen (human-on-the-loop) – mikä tarkoittaa, että jossain päätöksenteon vaiheessa ihmisyksilö pystyisi valvomaan tekoälyjärjestelmää tai puuttumaan sen toimintaan. Kun algoritmit osallistuvat päätöksentekoon esimerkiksi julkishallinnossa, yhteinen päätöksenteko voi kuitenkin olla hyvin monimutkaista ja hyvin hajautettua. Tekijöiden erottaminen toisistaan ja niihin puuttuminen tavalla, joka takaisi, että ihminen osallistuu prosessiin tai voi osallistua siihen, voi olla todella vaikeaa.

* **Toiseksi** teknologia voi myös olla suostuttelukykyistä: se vaikuttaa ihmisiin ja ohjaa heitä. Klassinen esimerkki on turvavöiden piippaava ääni. Jos turvavyötä ei ole kiinnitetty, se aiheuttaa monissa autoissa jatkuvan piippauksen. Tätä voidaan pitää eräänlaisena ohjaavana vaikutusvaltana – tässä tapauksessa eräänlaisena pakottamisena. Kuljettaja voi lopettaa äänen vain kiinnittämällä turvavyön. Nykyisillä algoritmisilla sovelluksilla voi olla yhä enemmän tällaisia ominaisuuksia – algoritmit ehdottavat, suosittavat ja rajoittavat vaihtoehtoja.

    Teko kuitenkin tehdään vapaaehtoisesti vain, jos se tehdään tarkoituksellisesti (se, joka toimii, päättää asiasta) ja ohjailevat vaikutteet eivät vaikuta siihen. Onko kuljettaja riippumaton ohjailevista vaikutteista, jos turvavyöjärjestelmä pakottaa hänet reagoimaan piippaukseen? Entä olemmeko vapaita ohjailusta, jos algoritmit päättävät, kenen kuvia näemme treffisivustoilla tai mitä musiikkia aiomme kuunnella? Mikä tarkalleen ottaen on ero algoritmisen ehdotuksen, hallinnan tai manipuloinnin välillä?

    Suostuttelukykyisen teknologian olisi luonnollisesti täytettävä vapaaehtoisuutta koskeva vaatimus autonomian takaamiseksi. Algoritmit monimutkaistavat tätä kysymystä, koska vapaaehtoisuus edellyttää riittävää ymmärrystä tietyn teknologian käytöstä. Mitä ”ymmärtää” itse asiassa tarkoittaa, ja mikä on riittävää? Millä ”ymmärrettävyyttä” mitataan – ”läpinäkyvyydellä”, ”selitettävyydellä” vai ”tarkastettavuudella”? Kuinka paljon käyttäjän tarkalleen ottaen pitäisi ymmärtää tekniikkaa, ja mitä hänen pitäisi ymmärtää siitä? Milloin voidaan aidosti arvioida, haluavatko käyttäjät käyttää kyseistä teknologiaa vai eivät? Tarkastelemme tätä aihetta tarkemmin luvussa 4.

</styled-text>

<quiz id="c40d3980-d9a4-5bb3-be35-aa497e1df7f5">

<img src="_MS_9489_HDR_cropped.jpg" alt="Hospital"> </img>

Kuva © Helsingin kaupunki / Viestintä

<br>
<br>

Palataan Helsingin terveydenhuoltoa koskevaan tapaukseen (jota käsiteltiin luvun 2 alussa). Oletetaan, että olet Helsingin kaupungin digijohtaja. Sinua pyydetään pohtimaan, pitäisikö kaupungin terveydenhuollon organisaation siirtyä reagoivasta terveydenhuollosta ehkäisevään terveydenhuoltoon. Luet erään raportin. Siinä kerrotaan uusista koneoppimisjärjestelmistä, jotka auttaisivat terveysviranomaisia ennustamaan kansalaisten mahdollisia terveysriskejä.

Raportissa mainitaan monia etuja, kuten sairauksien ehkäisy sekä parempi vaikutusten arviointi ja perusterveydenhuollon palvelujen suunnittelu. Raportissa tuodaan kuitenkin esiin myös huolenaiheita, jotka koskevat yksityisyyttä, polarisaatiota ja mahdollista tahattoman syrjinnän uhkaa. Lisäksi raportissa esitetään perustavanlaatuinen kysymys kaupungin roolista. Jos kaupungilla on tietoa mahdollisista terveysriskeistä, eikä se toimi tietojen perusteella, onko kaupunki moraalisesti vastuussa laiminlyönnistä?

Raportissa käsitellään myös vastuiden jakamista. Jos järjestelmää sovellettaisiin käytännössä, olisi aina olemassa vaara, että järjestelmä tekee virheen. Ketä pitäisi syyttää, jos näin tapahtuisi?

Lue ohjeet huolellisesti ja kirjoita vastauksesi seuraavaan tekstiruutuun. Muut käyttäjät ja opettajat arvioivat vastauksesi. Vastaa suomeksi. Tarkista vastauksesi ennen kuin napsautat Lähetä-painiketta, sillä et voi enää muokata vastaustasi lähettämisen jälkeen.

Vastaa seuraaviin digijohtajan vastuuta koskeviin kysymyksiin:

</quiz>

<quiz id="d4edd1dd-fa9c-5ab2-b65a-3f3e06c6a905"> </quiz>
