---
path: '/sv/chapter-3/4-the-problem-of-individuating-responsibilities'
title: 'Problemet med att individualisera ansvar'
hidden: false
---

<hero-icon heroIcon='chap3'/>


<styled-text>

Ansvar ses ofta som en persons eller organisations juridiska och etiska förpliktelse att bära ansvaret för användningen av AI-system, och att visa upp resultaten på ett transparent sätt. Den här formuleringen förutsätter att det finns en ”maktrelation”. Den individualiserar vem som bestämmer och vem som kan klandras.

Det har dock visat sig vara allmänt besvärligt att ange specifika kriterier för exakt hur ansvaret ska individualiseras, riktas och definieras. I många länder pågår diskussioner om dessa frågor. Internationella aktörer som EU och G7 har tagit upp dem som utmaningar som måste hanteras.

Varför är det så svårt att ange kriterier för vem som bär ansvaret?

* **För det första** finns det olika typer av ansvar. En agent ansvarar för en viss handling eller försummelse, men vad ansvaret består i beror på intressenten. Även om du genom att välja en handling kan uppfylla ditt ansvar, beror typen av ansvar på dina egenskaper. Intelligent teknik komplicerar detta ytterligare.

    Genom att delegera allt fler av våra beslut på algoritmer formar vi också strukturerna för beslutsfattande. AI förstärker vår intelligens genom att ge oss större beräkningskapacitet, vilket förbättrar vår förmåga att prognostisera och förstärker våra sinnen. Människa och maskin bildar en kognitiv hybrid. De samarbetar kognitivt (tänkande) och epistemiskt (kunskap), såväl individuellt som kollektivt. Detta skapar systemiska egenskaper.

    Ofta tänker man att det räcker att en människa i någon del av beslutsfattandet hålls informerad och kan övervaka och vid behov ingripa i det artificiella systemet. När algoritmer ger sig in i beslutsfattande i till exempel den offentliga sektorn, kan det kollektiva beslutsfattandet dock bli mycket komplext och vida spritt. Då kan det vara mycket svårt att individualisera och hantera faktorerna på ett sätt som håller en människa informerad om situationen.

* **För det andra** kan tekniken ta en insisterande form och börja påverka och kontrollera människor. Ett klassiskt exempel är ljudsignalen för säkerhetsbälten. I många bilar ljuder en konstant signal när man inte har satt på sig säkerhetsbältet. Det här kan ses som ett slags kontrollerande inflytande – det här fallet handlar det om ett slags tvång. Det enda sättet för föraren att stoppa signalen är att ta på sig säkerhetsbältet. Moderna algoritmtillämpningar kan ha fler och fler sådana funktioner: de föreslår och begränsar antalet alternativ.

    Men en handling är frivillig endast om den görs med avsikt (den som handlar har ”kontrollen”) och är fri från kontrollerande inflytande. Är föraren fri från kontrollerande inflytande om bältessystemet tvingar hen att reagera på ljudsignalen? Eller är vi fria från kontroll om algoritmerna bestämmer vilka bilder vi får se i dejtningsappar, eller vilken musik vi ska lyssna på? Exakt var går gränsen mellan algoritmbaserade förslag, kontroll och manipulation?

    Insisterande teknik måste givetvis uppfylla kravet på frivillighet för att autonomin ska kunna garanteras. Algoritmer komplicerar det här problemet eftersom frivilligheten förutsätter en tillräcklig förståelse för användningen av en viss teknik. Men vad menas med att ”förstå”, och vad utgör egentligen en tillräcklig grad? Vilken är rätt tolkning av begreppet ”förståelse”? Transparens? Förklarbarhet? Granskningsbarhet? Hur mycket, och exakt vad, behöver en användare förstå om en teknik? När kan vi göra en genuin uppskattning av huruvida användaren vill använda tekniken eller inte? Vi kommer att titta mer detaljerat på den här frågan i kapitel 4.

</styled-text>

<quiz id="92ae23ce-686c-5b6a-9960-ac408c6480ad">

<img src="_MS_9489_HDR_cropped.jpg" alt="Hospital"> </img>

Picture © City of Helsinki / Communications

<br>
<br>

Vi tar en ny titt på fallet med hälsovård i Helsingfors (som nämndes i början av kapitel 2). Föreställ dig att du är it-ansvarig för Helsingfors stad. Du blir tillfrågad om du tycker att stadens hälsovårdsorganisation ska gå över från reaktiv hälsovård till förebyggande hälsovård. Du läser en rapport. Den behandlar nya maskininlärningsbaserade metoder som kan hjälpa hälsovårdsmyndigheter att prognostisera möjliga hälsorisker för stadens invånare.

I rapporten nämns många fördelar, som till exempel sjukdomsförebyggande, bättre uppskattning av effekter och förbättrad planering av grundläggande hälsovårdstjänster. Rapporten talar dock även om vissa problem vad gäller integritet, polarisering och risken för oavsiktlig diskriminering. Dessutom behandlar rapporten stadens roll i ärendet. Om staden har information om potentiella hälsorisker och inte agerar baserat på dessa uppgifter, är staden då moraliskt ansvarig för försummelse?

Rapporten tar även upp frågan om individualiserat ansvar. Om systemet skulle tillämpas i praktiken finns det alltid en risk för att misstag skulle kunna uppstå. Vem kan vi klandra om detta skulle inträffa?

Läs anvisningarna noggrant och svara i textrutan nedan. Ditt svar kommer att granskas av andra användare och av handledarna. Svara på engelska, och kontrollera svaret innan du skickar det. När du har skickat svaret kan du inte längre göra några ändringar.

Svara på följande frågor om it-chefens ansvar:

</quiz>

<quiz id="3d29591c-0026-52fd-9b10-cbf1d992ba2c"> </quiz>
