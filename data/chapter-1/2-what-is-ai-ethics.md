---
path: '/chapter-1/2-what-is-ai-ethics'
title: 'What is AI ethics?'
hidden: false
---

#### On ethical guidelines

Before looking at AI ethics, we need to set out what ethics means in the first place.

Ethics seeks to answer questions like “what is good or bad?”, “what is right - what is wrong”, or “what is
justice, well-being or equality”. As a discipline, ethics involves systematizing, defending, and recommending
concepts of right and wrong conduct by using [conceptual analysis](https://en.wikipedia.org/wiki/Philosophical_analysis), [thought experiments](https://plato.stanford.edu/entries/thought-experiment/), and [argumentation](https://iep.utm.edu/argument/). (If you want to know more about philosophical reasoning, see this [video](https://www.youtube.com/watch?v=NKEhdsnKKHs) by Crash Course
Philosophy.)

<text-box icon="philIcon" background="rgba(246, 235, 232, 0.5)" name="The three subfields of ethics">

**1) Meta-ethics** studies the meaning of ethical concepts, the existence of ethical entities
(ontology) and the possibility of ethical knowledge (epistemology).

**2) Normative ethics** concerns the practical means of determining a moral (or ethically
correct) course of action

**3) Applied ethics** concerns what a moral agent (defined as someone who can judge what is
right and wrong and be held accountable) is obligated or permitted to do in a specific
situation or a particular domain of action.

</text-box>

AI ethics is a subfield of applied ethics. Nowadays, AI ethics is considered as part of the ethics of
technology specific to robots and other artificially intelligent entities. It concerns the questions of how
developers, manufacturers, authorities and operators should behave in order to minimise the ethical risks
that can arise from AI in society, either arising from design, inappropriate application or intentional misuse
of the technology.

These concerns can be divided into three timeframes as follows:
* immediate, here-and-now questions about, for instance, security, privacy or transparency in AI
systems
* medium-term concerns about, for instance, the impact of AI on the military use, medical care, or
justice and educational systems
* longer-term concerns about the fundamental ethical goals of developing and implementing AI in
society

<text-box icon="bgIcon" background="rgba(224, 234, 235, 0.5)" name="From machine ethics to the ethics of AI">

For a long time, AI ethics was taken to mean mostly machine and roboethics. They study the ethical codes
of possible artificial moral agents. As research fields, they are based on a scenario that machines can one
day be responsible for ethically relevant choices, and can be even considered possibly as ethical agents
(Anderson and Anderson 2007) or ‘autonomous moral agents’ (van Wynsberghe and Robbins forthcoming).
As a comparison, animals are generally not considered moral agents. We don’t judge a squirrel's behaviour
as right or wrong, and we don’t ascribe them the capacity to know the difference.

Machine and roboethics span from the development of ethically responsive autonomous vehicles to the
design of ethical codes for moral autonomous agents.

Isaac Asimov famously proposed ‘three laws of robotics’ that would guide the moral action of machines
(Asimov 1942):

- A robot may not injure a human being or, through inaction, allow a human being to come to harm.

- A robot must obey the orders given it by human beings except where such orders would conflict
with the First Law.

- A robot must protect its own existence as long as such protection does not conflict with the First or
Second Laws.

</text-box>

These days, AI ethics is a more general field, and closer to engineering ethics: we don’t have to assume the
machine is an ethical agent, to analyse its ethics. Research in the field of AI ethics ranges from reflections
on how ethical or moral principles can be implemented in autonomous machines (Anderson and Anderson
2015), to the empirical analysis on how trolley problems are solved (Awad et al. 2018), the systematic
analysis of ethical principles, such as fairness (Jobin et al. 2019) and the critical evaluation of ethical
frameworks.
